{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barshay Secondary Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After not really understanding how to turn my cosine similarity results into a classification model, I was at somewhat of a crossroads. One idea that was mentioned in class that I decided to use as my first real attempt at classifcation was to make a vector out of tfidf vectorized features where each vector contained: the question text, the document text, as well as each possible long answer (separately). So if a question had 50 possible long answers, then there would be 50 documents corresponding to each question where the only difference between the different vectors is their potential long answer. The idea behind this was to allow the machine learning model to see some of the similarities behind each vector, so as to highlight the differences that each vector contains in terms of its potential long answer. The hope was that this strategy would give more context to the vectors and thus hopefully improve prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that sounds somewhat confusing let me walk you through an example. I start with importing all my favorite data science tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Much of the structure from this sample was borrowed from Dr. Paul Anderson.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had initially started with many more observations being loaded in however, my computer couldn't handle it and so I had to go down to 100, which is unfortunate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_100 = open(\"/Users/mdbarshay/Desktop/DATA 301 Project/tensorflow2-question-answering/simplified-nq-train.jsonl\")\n",
    "records_train = [] \n",
    "c=0\n",
    "for line in train_100:\n",
    "    records_train.append(json.loads(line))\n",
    "    if c > 99:\n",
    "        break\n",
    "    c+=1\n",
    "train_df = pd.DataFrame(records_train)\n",
    "train_100.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are defining a function that helps to process the data to make it ameable for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes each row and normalizes each json object within each row. In this case it is normalizing the long answer candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(row):\n",
    "    tdf = json_normalize(row['long_answer_candidates'])\n",
    "    tdf[\"example_id\"] = row['example_id'] \n",
    "    return tdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply this function to our training data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_answer_candidates = train_df.apply(process,axis=1)\n",
    "long_answer_candidates = pd.concat(long_answer_candidates.values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is somewhat similar to the strategies that I have used before to get each long answer by itself, but this one is especially nice since it contains the example id, which can be used in the future for keeping track of which question the tokens belong to. Let me just confirm that this function worked as hoped by ensuring that we got all of the long answer tokens corresponding to id #: 5655493461695504401. I do so below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(long_answer_candidates[long_answer_candidates[\"example_id\"] == 5655493461695504401])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0][\"long_answer_candidates\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just for reference I will quickly calculate how many long answers candidates correspond to each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The average number of long answer candidates for each question is 107.14851485148515 candidates.'"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The average number of long answer candidates for each question is \" + str(len(long_answer_candidates) / 101) \\\n",
    "+ \" candidates.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to do a similar normalizing process for the annotations, I define a similar process function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(row):\n",
    "    tdf = json_normalize(row['annotations'])\n",
    "    tdf[\"example_id\"] = row['example_id']  \n",
    "    return tdf\n",
    "\n",
    "annotations = train_df.apply(process,axis=1)\n",
    "annotations = pd.concat(annotations.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, I am normalizing the short answers, but this time I have to be careful to deal with special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(row):\n",
    "    tdf = json_normalize(row['short_answers'])\n",
    "    tdf[\"example_id\"] = row['example_id']\n",
    "    for c in row.index:\n",
    "        if c == \"short_answers\":\n",
    "            continue\n",
    "        tdf[c] = row.loc[c]\n",
    "    return tdf\n",
    "\n",
    "short_answers = annotations.apply(process,axis=1)\n",
    "short_answers = pd.concat(short_answers.values,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is time to put all of the pieces together and join everything back into one data frame, making sure to set the index so that I am able to not have duplicate observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_train_df = train_df.set_index(\"example_id\").join(long_answer_candidates.set_index(\"example_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I remove all of the characters that can be found between \"<\" and \">\" which effectively means removing all HTML tags.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_text_token = joined_train_df.apply(lambda row: re.sub(\"<.*?>\", \" \",\" \".join(str(row['document_text']).split()[row.start_token:row.end_token]).lower()),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines give us a data frame where the first column is the example id and the second column is the potential answer corresponding to one of the many potential answers for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_text_token = document_text_token.reset_index()\n",
    "document_text_token.columns = [\"example_id\",\"document_text_token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to fit the tfidf vectorizer. I am going to fit the vectorizer on the document_text_token column of the above data frame. For this tfidf vectorizer each document is going to correspond to one potential long answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text_vectorizer = TfidfVectorizer()\n",
    "doc_text_vectorizer.fit(document_text_token[\"document_text_token\"]) #only fit tfidf on the document text not the id\n",
    "X1 = doc_text_vectorizer.transform(document_text_token[\"document_text_token\"]) #save the results to X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a sparse matrix that has as its rows each potential long answer and as its columns the vocabulary that it has learned. Now is time to fit another tfidf vectorizer, this time on the questions. However, we treat the questions slightly differently due to the fact that there are many repeats in the questions so no point fitting on duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10822x464 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 90460 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question_vectorizer = TfidfVectorizer()\n",
    "question_vectorizer.fit(joined_train_df[\"question_text\"].drop_duplicates())\n",
    "X2 = question_vectorizer.transform(joined_train_df[\"question_text\"])\n",
    "display(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last piece of the puzzle is to include the document text in these vectors along with everything else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = joined_train_df.document_text\n",
    "full_text = full_text.apply(lambda row : re.sub(\"<.*?>\", \" \",\" \".join(str(row).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = full_text.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vectorizer = TfidfVectorizer()\n",
    "full_vectorizer.fit(full_text[\"document_text\"].drop_duplicates())\n",
    "X3 = full_vectorizer.transform(full_text[\"document_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10822x25720 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 269295 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<10822x464 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 90460 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<10822x35038 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17007718 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X1)\n",
    "display(X2)\n",
    "display(X3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three sparse matrices all have the same number of rows. I believe that X1 should have a slightly smaller vocabulary than X3, and also X3 should have many more filled in observations since it vectorized the entire document rather than just a particular sentence. Unfortunately I believe that each column corresponds to the vocabulary that was in that run of tfidf vectorizer. This means that there are repeat columns for the same words across the different groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now going to stack all of these observations horizontally so that each row still corresponds to a single observations, but now all of the matrices that I made are going to be side by side. In this setup a row is a vector corresponding to a possible long answer. I define my X_train below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack((X1,X2,X3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10822x61222 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17367473 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the number of rows did not change, however we see that the number of columns did change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10822, 61222)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to create the target data that is going to help us train a model to make predictions with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now go back to one of our previous data frames and add a column corresponding to whether or not that specific question was the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier on ourselves lets set the index of annotations to example_id. The next bit of code something I discovered while trying to correctly get the correct question indexes to line up, there is no purpose for it other than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.sort_values(by = \"example_id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.set_index(\"example_id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_answer_candidates.set_index(\"example_id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_long_answer_candidates = long_answer_candidates.copy()\n",
    "fixed_long_answer_candidates.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_long_answer_candidates = fixed_long_answer_candidates.sort_values(by = [\"example_id\", \"start_token\"])\n",
    "fixed_long_answer_candidates.index = range(len(long_answer_candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function that I created to extract out the correct long answer, basically it involves looping through the fixed long answer candidates and comparing the potential long answers to the correct ones. After the function I print out the first 10 things in the list. Surprisingly (but correclty, I checked) the first observation is a one meaning the first observation in fixed_long_answer_candidates represents a correct potential answer. If the question had no answer then a 0 will be appended to that list since no potential long answers have -1 as their start index, which is what all questions that have no long answer have in their annotations section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_assessment = []\n",
    "for i in range(len(fixed_long_answer_candidates)):\n",
    "    ex_id = fixed_long_answer_candidates.loc[i, \"example_id\"]\n",
    "    correct_start = annotations.loc[ex_id, \"long_answer.start_token\"]\n",
    "    correct_end = annotations.loc[ex_id, \"long_answer.end_token\"]\n",
    "    if (fixed_long_answer_candidates.loc[i, \"start_token\"] == correct_start) & (fixed_long_answer_candidates.loc[i, \"end_token\"] == correct_end):\n",
    "        accuracy_assessment.append(1)\n",
    "    else:\n",
    "        accuracy_assessment.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_assessment[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = accuracy_assessment.copy() # just making a copy so it is named y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to use cross val score to get some estimates for my test error. The first model that I am going to train is is KNeighborsClassifier. I chose to use f1 score as my scoring mechanism since it incorporates both precision and recall, and I chose the number of folds to be three since I know cross_val_score can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=9, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "res = cross_val_score(model, X_train, y_train, cv = 3, scoring = \"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all three folds had an f1 score of 0. This means that it is time to break out some more complicated models. I decided to first try catboost with balanced weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(res).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "v = pd.Series(y_train).value_counts().values\n",
    "class_weights = [1,v[0]/v[1]]\n",
    "\n",
    "model = CatBoostClassifier(iterations=100,\n",
    "                          learning_rate=1,\n",
    "                          depth=8,\n",
    "                          class_weights=class_weights,\n",
    "                          verbose = False)\n",
    "# with cross_val_score we do not have to fit the model at this stage since cross val actually fits a model at \n",
    "# each step of its cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_val_score(model, X_train, y_train, cv = 2, scoring = \"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our average level of precision was 0.020585880154811158 this not a very good result.'"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Our average level of precision was \" +  str(res.mean()) + \" this not a very good result.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "resR = cross_val_score(model, X_train, y_train, cv = 2, scoring = \"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our average level of recall was 0.12698412698412698 this not a great result.'"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Our average level of recall was \" +  str(resR.mean()) + \" this not a great result.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this result is slighty better than we were able to do with precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',class_weight=\"balanced\")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our recall and precision levels were very lackluster. We were unable to predict any observations correctly on this trial. I am starting to get the idea that catboost is likely the best that we can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_val_score(clf, X_train, y_train, cv = 3, scoring = \"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_val_score(clf, X_train, y_train, cv = 3, scoring = \"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cannot explain why we get these values for Logistic Regression with these set of features, but logistic regression turns out to be possibly the best model with other sets of features, some things in data science are mysteries. It is not because of my choice for class_weight balanced, we got the same result regardless of the class weight in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with these choice of features is its computational expense. My computer gets warm and the fan gets loud... there must be a way that we can keep these results or improve while being more efficient, or at the very least smarter about what we are doing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
