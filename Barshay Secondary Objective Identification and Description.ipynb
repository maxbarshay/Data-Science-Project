{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Barshay's Secondary Objective: \n",
    "\n",
    "For my secondary objective I decided to try to investigate the differences between different questions that were asked to try to see if I could cluster them by \"genre\". I wanted to pick an unsupervised problem because I knew that a lot of what I was going to be doing for the primary objective was very structured and supervised, we had to follow the rules of the competition. What I was thinking as I looked through the different questions was that some seemed to be related to sports, some related to science etc. I was hoping that I could use clustering to pick up on these differences in an automated fashion. At the time I did not really consider the fact that no having labels could make evaluation difficult. The first road block that I ran into was my choice of vectorization for each word. I could use a count vectorizer or I could use a tf-idf vectorizer which would give more weight to less frequently seen words.\n",
    "    \n",
    "The first method that I tried was a count vectorizer since I figured it was more simple and easy to deal with, however this gave me some strange patterns when I ran PCA on the various groups, likely due to the fact that the sentences varied in the number of words that they contained and count vectorizer gives words weights in a data frame based on their (integer) occurences in the text. This is in contrast to tf-idf vectorizer which primarily gives float values. We have seen that discrete variables and PCA do not work very well together in prior labs and elsewhere throughout this class.\n",
    "    \n",
    "I then performed many various iterations of PCA and clusterings, both separately and together, however it was all pretty much to no avail. At that point I did some searching online and discovered a relatively easy to use NLP package called spacy. I began to show some simple examples of spacys power and functionality and wrote some functions to go with it. The second half of my second objective I continued to try to group the questions by their content, but this time I was using the more advanced spacy vectors rather than just doing it all from scratch. This turned out to be a very good idea, however even using their pre-determined vectors there were many things that I had to implement from scratch. I discovered some things that seemed to improve results and took advantage of them and made changes in an iterative fashion. However, since this was an unsupervised task there was no definitive score that I could give myself. I think that you will agree that my results improved after my iterative improvements, but you will have to decide that for yourself.\n",
    "    \n",
    "My last major accomplishment of this second objective was performing PCA on the mean vectors of each question text and then doing clustering on the PCA. This probably sounds a lot like that I was describing in the last 2 paragraphs, however at this point I discovered two things that significantly increased my models \"accuracy\" (subjective) and I felt like not only were the most common words in each group meaningful words, but they seemed to all be related, within each group, and somewhat different across different groups just as I had hoped.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
