{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results (more like results part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I finished my secondary results and viewed the sample that you sent I wanted to give another method a shot. I build off a lot of things that I learned from that part of the project in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ended up only importing 1,000 lines so that I could run these models more quickly. It turns out that mapping the partitions is extremely computationally expensive, so I did it once with 1000 and was able to store it in joblib. The steps below are all pre-processing steps that are from the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Much of the structure from this sample was borrowed from Dr. Paul Anderson.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import pandas as pd\n",
    "import pandas.io.json\n",
    "import json\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are loading in the data with dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_text</th>\n",
       "      <th>long_answer_candidates</th>\n",
       "      <th>question_text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_url</th>\n",
       "      <th>example_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=19</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 57 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               document_text long_answer_candidates question_text annotations document_url example_id\n",
       "npartitions=19                                                                                       \n",
       "                      object                 object        object      object       object      int64\n",
       "                         ...                    ...           ...         ...          ...        ...\n",
       "...                      ...                    ...           ...         ...          ...        ...\n",
       "                         ...                    ...           ...         ...          ...        ...\n",
       "                         ...                    ...           ...         ...          ...        ...\n",
       "Dask Name: from-delayed, 57 tasks"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask.config.set({'temporary_directory': '/disk/tmp'})\n",
    "pd.io.json._json.loads = lambda s, *a, **kw: json.loads(s)\n",
    "train_df = dd.read_json(\"simplified-nq-train.jsonl.sample_small\",lines=True, blocksize=3e6)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_text</th>\n",
       "      <th>long_answer_candidates</th>\n",
       "      <th>question_text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_url</th>\n",
       "      <th>example_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Email marketing - Wikipedia &lt;H1&gt; Email marketi...</td>\n",
       "      <td>[{'start_token': 14, 'top_level': True, 'end_t...</td>\n",
       "      <td>which is the most common use of opt-in e-mail ...</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Em...</td>\n",
       "      <td>5655493461695504401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The Mother ( How I Met Your Mother ) - wikiped...</td>\n",
       "      <td>[{'start_token': 28, 'top_level': True, 'end_t...</td>\n",
       "      <td>how i.met your mother who is the mother</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Th...</td>\n",
       "      <td>5328212470870865242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Human fertilization - wikipedia &lt;H1&gt; Human fer...</td>\n",
       "      <td>[{'start_token': 14, 'top_level': True, 'end_t...</td>\n",
       "      <td>what type of fertilisation takes place in humans</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Hu...</td>\n",
       "      <td>4435104480114867852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>List of National Football League career quarte...</td>\n",
       "      <td>[{'start_token': 28, 'top_level': True, 'end_t...</td>\n",
       "      <td>who had the most wins in the nfl</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Li...</td>\n",
       "      <td>5289242154789678439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Roanoke Colony - wikipedia &lt;H1&gt; Roanoke Colony...</td>\n",
       "      <td>[{'start_token': 32, 'top_level': True, 'end_t...</td>\n",
       "      <td>what happened to the lost settlement of roanoke</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Ro...</td>\n",
       "      <td>5489863933082811018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       document_text  \\\n",
       "0  Email marketing - Wikipedia <H1> Email marketi...   \n",
       "1  The Mother ( How I Met Your Mother ) - wikiped...   \n",
       "2  Human fertilization - wikipedia <H1> Human fer...   \n",
       "3  List of National Football League career quarte...   \n",
       "4  Roanoke Colony - wikipedia <H1> Roanoke Colony...   \n",
       "\n",
       "                              long_answer_candidates  \\\n",
       "0  [{'start_token': 14, 'top_level': True, 'end_t...   \n",
       "1  [{'start_token': 28, 'top_level': True, 'end_t...   \n",
       "2  [{'start_token': 14, 'top_level': True, 'end_t...   \n",
       "3  [{'start_token': 28, 'top_level': True, 'end_t...   \n",
       "4  [{'start_token': 32, 'top_level': True, 'end_t...   \n",
       "\n",
       "                                       question_text  \\\n",
       "0  which is the most common use of opt-in e-mail ...   \n",
       "1            how i.met your mother who is the mother   \n",
       "2   what type of fertilisation takes place in humans   \n",
       "3                   who had the most wins in the nfl   \n",
       "4    what happened to the lost settlement of roanoke   \n",
       "\n",
       "                                         annotations  \\\n",
       "0  [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "1  [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "2  [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "3  [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "4  [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "\n",
       "                                        document_url           example_id  \n",
       "0  https://en.wikipedia.org//w/index.php?title=Em...  5655493461695504401  \n",
       "1  https://en.wikipedia.org//w/index.php?title=Th...  5328212470870865242  \n",
       "2  https://en.wikipedia.org//w/index.php?title=Hu...  4435104480114867852  \n",
       "3  https://en.wikipedia.org//w/index.php?title=Li...  5289242154789678439  \n",
       "4  https://en.wikipedia.org//w/index.php?title=Ro...  5489863933082811018  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train_df = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a tfidf vectorizer on the question text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_VEC=True\n",
    "if LOAD_VEC:\n",
    "    question_vectorizer = joblib.load(\"question_vectorizer.joblib.z\")\n",
    "else:\n",
    "    question_vectorizer = TfidfVectorizer()\n",
    "    question_vectorizer.fit(train_df[\"question_text\"])\n",
    "    joblib.dump(question_vectorizer,\"question_vectorizer.joblib.z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a tfidf vectorizer on the document text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "if LOAD_VEC:\n",
    "    document_vectorizer = joblib.load(\"document_vectorizer.joblib.z\")\n",
    "else:\n",
    "    document_vectorizer = TfidfVectorizer()\n",
    "    train_df[\"document_text_no_html\"] = train_df[\"document_text\"].apply(lambda value: re.sub(\"<.*?>\", \"\",value))\n",
    "    document_vectorizer.fit(train_df[\"document_text_no_html\"])\n",
    "    joblib.dump(document_vectorizer,\"document_vectorizer.joblib.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We have 165376 words in our vocabulary.'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"We have \" + str(len(document_vectorizer.get_feature_names())) + \" words in our vocabulary.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating our features to load into our soon to be made feature vectors. Note that this way of doing things is in stark contrast to the way that I was doing things in my *Barshay Secondary Results*. My feature vector had 61,222 columns as opposed to this one which was only 17 columns. You can imagine that the machine learning models are able to perform much faster on this model. The biggest difference is that on this model we are really doing the processing before it is time to turn it into a vector and then using those results to make a vector as opposed to just putting all the raw information in a vector all at once. There are also other significant structural differences, namely the fact that this model is in the form of a pandas dataframe which allows us to do maniupulations that we would not be able to do with a large sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import sklearn\n",
    "import Levenshtein \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Function based on one found at https://www.kaggle.com/opanichev/tf2-0-qa-binary-classification-baseline\n",
    "\n",
    "def extract_features(document_tfidf, question_tfidf, answer_tfidf, document, question, answer, y):\n",
    "    # needed to convert to vectors\n",
    "    document_tfidf = np.squeeze(np.asarray(document_tfidf))\n",
    "    question_tfidf = np.squeeze(np.asarray(question_tfidf))\n",
    "    answer_tfidf = np.squeeze(np.asarray(answer_tfidf))\n",
    "    qa_cos_d = spatial.distance.cosine(question_tfidf, answer_tfidf)\n",
    "    qd_cos_d = spatial.distance.cosine(question_tfidf, document_tfidf)\n",
    "    ad_cos_d = spatial.distance.cosine(answer_tfidf, document_tfidf)\n",
    "\n",
    "    qa_euc_d = np.linalg.norm(question_tfidf - answer_tfidf)\n",
    "    qd_euc_d = np.linalg.norm(question_tfidf - document_tfidf)\n",
    "    ad_euc_d = np.linalg.norm(answer_tfidf - document_tfidf)\n",
    "    \n",
    "    qa_lev_d = Levenshtein.distance(question, answer)\n",
    "    qa_lev_r = Levenshtein.ratio(question, answer)\n",
    "    qa_jar_s = Levenshtein.jaro(question, answer) \n",
    "    qa_jaw_s = Levenshtein.jaro_winkler(question, answer)\n",
    "    \n",
    "    qa_tfidf_score = np.sum(question_tfidf*answer_tfidf.T)\n",
    "    qd_tfidf_score = np.sum(question_tfidf*document_tfidf.T)\n",
    "    ad_tfidf_score = np.sum(answer_tfidf*document_tfidf.T)\n",
    "    \n",
    "    document_tfidf_sum = np.sum(document_tfidf)\n",
    "    question_tfidf_sum = np.sum(question_tfidf)\n",
    "    answer_tfidf_sum = np.sum(answer_tfidf)\n",
    "    \n",
    "    f = pd.Series([\n",
    "        qa_cos_d, qd_cos_d, ad_cos_d, \n",
    "        qa_euc_d, qd_euc_d, ad_euc_d,\n",
    "        qa_lev_d, qa_lev_r, qa_jar_s, qa_jaw_s,\n",
    "        qa_tfidf_score, qd_tfidf_score, ad_tfidf_score, \n",
    "        document_tfidf_sum, question_tfidf_sum, answer_tfidf_sum,y\n",
    "    ],index=['qa_cos_d', 'qd_cos_d', 'ad_cos_d', \n",
    "        'qa_euc_d', 'qd_euc_d', 'ad_euc_d',\n",
    "        'qa_lev_d', 'qa_lev_r', 'qa_jar_s', 'qa_jaw_s',\n",
    "        'qa_tfidf_score', 'qd_tfidf_score', 'ad_tfidf_score', \n",
    "        'document_tfidf_sum', 'question_tfidf_sum', 'answer_tfidf_sum','candidate'])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions are more processing steps on the data to get it ready to be put through models. The main takeaways of these functions are processing and applying the function extract features the was defined above. Like I said before these choice of features are much different from the previous choice of features in the sense that most if not all of the calculations are done *prior* to creating the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "\n",
    "v = document_vectorizer # shorter code below\n",
    "\n",
    "def process(row,column):\n",
    "    columns = list(row.index)\n",
    "    columns.remove(column)\n",
    "    tdf = json_normalize(row.to_dict(),column,meta=columns)\n",
    "    tdf = tdf.reset_index()\n",
    "    tdf.rename(columns={('index'): (tdf.columns[0]+\":\"+column)}, inplace=True)\n",
    "    return tdf\n",
    "\n",
    "def on_partition(df):\n",
    "    document_text = df.apply(lambda row: re.sub(\"<.*?>\", \"HTML_TAG\",row.loc[\"document_text\"]),axis=1) #replacing\n",
    "    # html tages with HTML_TAG so that the model can still understand the context that the HTML tag created.\n",
    "    # There are so many little things that you need to decide on that can make significant differences\n",
    "    df[\"document_text\"] = document_text\n",
    "    annotation = json_normalize(df[\"annotations\"].apply(lambda x: x[0]).values)\n",
    "    annotation['example_id'] = df['example_id']\n",
    "    annotation.set_index('example_id',inplace=True)\n",
    "    df2_as_series = df.apply(lambda row: process(row,\"long_answer_candidates\"),axis=1)\n",
    "    df2 = pd.concat(df2_as_series.values)\n",
    "    df2 = df2.loc[df2.top_level == True]\n",
    "    df3 = df2.set_index('example_id').join(annotation)\n",
    "    df3[\"candidate\"] = df3[\"long_answer.candidate_index\"] == df3[\"index:long_answer_candidates\"]\n",
    "    answer_text = df3.apply(lambda row: \" \".join(row.loc[\"document_text\"].split()[row.loc[\"start_token\"]:row.loc[\"end_token\"]]),axis=1)\n",
    "    df3[\"answer_text\"] = answer_text\n",
    "    features = df3.apply(lambda row: extract_features(v.transform([row.loc[\"document_text\"]]).todense(),\n",
    "                                                      v.transform([row.loc[\"question_text\"]]).todense(),\n",
    "                                                      v.transform([row.loc[\"answer_text\"]]).todense(), \n",
    "                                                      row.loc[\"document_text\"], \n",
    "                                                      row.loc[\"question_text\"], \n",
    "                                                      row.loc[\"answer_text\"],row.loc[\"candidate\"]),axis=1)\n",
    "    return features\n",
    "\n",
    "for_meta = on_partition(train_df.head())\n",
    "\n",
    "\n",
    "if LOAD_VEC: # I do not want to run the dask's map_partitions more than I have to\n",
    "    pass\n",
    "else:\n",
    "    train_df2 = train_df.map_partitions(on_partition,meta=for_meta).compute(scheduler=\"processes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39192, 17)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_cos_d</th>\n",
       "      <th>qd_cos_d</th>\n",
       "      <th>ad_cos_d</th>\n",
       "      <th>qa_euc_d</th>\n",
       "      <th>qd_euc_d</th>\n",
       "      <th>ad_euc_d</th>\n",
       "      <th>qa_lev_d</th>\n",
       "      <th>qa_lev_r</th>\n",
       "      <th>qa_jar_s</th>\n",
       "      <th>qa_jaw_s</th>\n",
       "      <th>qa_tfidf_score</th>\n",
       "      <th>qd_tfidf_score</th>\n",
       "      <th>ad_tfidf_score</th>\n",
       "      <th>document_tfidf_sum</th>\n",
       "      <th>question_tfidf_sum</th>\n",
       "      <th>answer_tfidf_sum</th>\n",
       "      <th>candidate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>-8799945603687418006</td>\n",
       "      <td>0.766622</td>\n",
       "      <td>0.815032</td>\n",
       "      <td>0.261347</td>\n",
       "      <td>1.238242</td>\n",
       "      <td>1.276739</td>\n",
       "      <td>0.722976</td>\n",
       "      <td>442</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.398669</td>\n",
       "      <td>0.398669</td>\n",
       "      <td>0.233378</td>\n",
       "      <td>0.184968</td>\n",
       "      <td>0.738653</td>\n",
       "      <td>6.322635</td>\n",
       "      <td>2.069796</td>\n",
       "      <td>4.247175</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-8799945603687418006</td>\n",
       "      <td>0.777648</td>\n",
       "      <td>0.815032</td>\n",
       "      <td>0.142970</td>\n",
       "      <td>1.247115</td>\n",
       "      <td>1.276739</td>\n",
       "      <td>0.534734</td>\n",
       "      <td>122</td>\n",
       "      <td>0.226190</td>\n",
       "      <td>0.444032</td>\n",
       "      <td>0.444032</td>\n",
       "      <td>0.222352</td>\n",
       "      <td>0.184968</td>\n",
       "      <td>0.857030</td>\n",
       "      <td>6.322635</td>\n",
       "      <td>2.069796</td>\n",
       "      <td>2.957377</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-8799945603687418006</td>\n",
       "      <td>0.996504</td>\n",
       "      <td>0.815032</td>\n",
       "      <td>0.271353</td>\n",
       "      <td>1.411740</td>\n",
       "      <td>1.276739</td>\n",
       "      <td>0.736686</td>\n",
       "      <td>1693</td>\n",
       "      <td>0.025258</td>\n",
       "      <td>0.408574</td>\n",
       "      <td>0.408574</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>0.184968</td>\n",
       "      <td>0.728647</td>\n",
       "      <td>6.322635</td>\n",
       "      <td>2.069796</td>\n",
       "      <td>6.097632</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-8627347779381584683</td>\n",
       "      <td>0.983627</td>\n",
       "      <td>0.416655</td>\n",
       "      <td>0.941501</td>\n",
       "      <td>1.402589</td>\n",
       "      <td>0.912858</td>\n",
       "      <td>1.372225</td>\n",
       "      <td>301</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.396555</td>\n",
       "      <td>0.396555</td>\n",
       "      <td>0.016373</td>\n",
       "      <td>0.583345</td>\n",
       "      <td>0.058499</td>\n",
       "      <td>13.708678</td>\n",
       "      <td>2.396368</td>\n",
       "      <td>5.156949</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-8627347779381584683</td>\n",
       "      <td>0.792941</td>\n",
       "      <td>0.416655</td>\n",
       "      <td>0.682364</td>\n",
       "      <td>1.259318</td>\n",
       "      <td>0.912858</td>\n",
       "      <td>1.168215</td>\n",
       "      <td>1086</td>\n",
       "      <td>0.055507</td>\n",
       "      <td>0.329138</td>\n",
       "      <td>0.329138</td>\n",
       "      <td>0.207059</td>\n",
       "      <td>0.583345</td>\n",
       "      <td>0.317636</td>\n",
       "      <td>13.708678</td>\n",
       "      <td>2.396368</td>\n",
       "      <td>6.611733</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      qa_cos_d  qd_cos_d  ad_cos_d  qa_euc_d  qd_euc_d  \\\n",
       "example_id                                                               \n",
       "-8799945603687418006  0.766622  0.815032  0.261347  1.238242  1.276739   \n",
       "-8799945603687418006  0.777648  0.815032  0.142970  1.247115  1.276739   \n",
       "-8799945603687418006  0.996504  0.815032  0.271353  1.411740  1.276739   \n",
       "-8627347779381584683  0.983627  0.416655  0.941501  1.402589  0.912858   \n",
       "-8627347779381584683  0.792941  0.416655  0.682364  1.259318  0.912858   \n",
       "\n",
       "                      ad_euc_d  qa_lev_d  qa_lev_r  qa_jar_s  qa_jaw_s  \\\n",
       "example_id                                                               \n",
       "-8799945603687418006  0.722976       442  0.081800  0.398669  0.398669   \n",
       "-8799945603687418006  0.534734       122  0.226190  0.444032  0.444032   \n",
       "-8799945603687418006  0.736686      1693  0.025258  0.408574  0.408574   \n",
       "-8627347779381584683  1.372225       301  0.153846  0.396555  0.396555   \n",
       "-8627347779381584683  1.168215      1086  0.055507  0.329138  0.329138   \n",
       "\n",
       "                      qa_tfidf_score  qd_tfidf_score  ad_tfidf_score  \\\n",
       "example_id                                                             \n",
       "-8799945603687418006        0.233378        0.184968        0.738653   \n",
       "-8799945603687418006        0.222352        0.184968        0.857030   \n",
       "-8799945603687418006        0.003496        0.184968        0.728647   \n",
       "-8627347779381584683        0.016373        0.583345        0.058499   \n",
       "-8627347779381584683        0.207059        0.583345        0.317636   \n",
       "\n",
       "                      document_tfidf_sum  question_tfidf_sum  \\\n",
       "example_id                                                     \n",
       "-8799945603687418006            6.322635            2.069796   \n",
       "-8799945603687418006            6.322635            2.069796   \n",
       "-8799945603687418006            6.322635            2.069796   \n",
       "-8627347779381584683           13.708678            2.396368   \n",
       "-8627347779381584683           13.708678            2.396368   \n",
       "\n",
       "                      answer_tfidf_sum  candidate  \n",
       "example_id                                         \n",
       "-8799945603687418006          4.247175       True  \n",
       "-8799945603687418006          2.957377      False  \n",
       "-8799945603687418006          6.097632      False  \n",
       "-8627347779381584683          5.156949      False  \n",
       "-8627347779381584683          6.611733      False  "
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_df2.joblib.z']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_df2,\"train_df2.joblib.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_VEC:\n",
    "    train_df2 = joblib.load(\"train_df2.joblib.z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training \n",
    "## Throwing the kitchen sink at this data\n",
    "\n",
    "Now that we got the hard work out of the way it is time to do our last data pre-processing steps. It is time to split the data into test and training sets. First we make sure that there are no missing values. Since we want all observations corresponding to certain ID's (questions) to be classified together we first use np.unique to get a np array of the unique indexes. We then use train_test_split to split the data where 66% of the unique ID's are going to be training data while the other 33% are going to be testing data. This way all questions will be grouped into the same groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_df2.fillna(0)\n",
    "example_ids = np.unique(train_df2.index)\n",
    "\n",
    "example_ids_train, example_ids_test = train_test_split(example_ids, test_size=0.33, shuffle=True, random_state=42)\n",
    "X_train = X.loc[example_ids_train]\n",
    "X_test = X.loc[example_ids_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am making distinct data frames to represent both the test and the training data so that it is easier to train models (and also less confusing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[\"candidate\"] # only run this cell once\n",
    "X_train.drop(\"candidate\", axis = 1, inplace = True)\n",
    "y_test = X_test[\"candidate\"]\n",
    "X_test.drop(\"candidate\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am running cat boost and manually changing my class weights setting the not correct answer group to 1 and the the correct answer group to the ratio between correct and incorrect groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7ffe8902ead0>"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "y_train *= 1 # this simply converts the trues and falses into ones and zeroes using pythons built in ability to store\n",
    "# true values as one and false values as zero\n",
    "v = y_train.value_counts().values\n",
    "class_weights = [1,v[0]/v[1]]\n",
    "\n",
    "# Initialize CatBoostRegressor\n",
    "model = CatBoostClassifier(iterations=100,\n",
    "                          learning_rate=1,\n",
    "                          depth=8,\n",
    "                          class_weights=class_weights,\n",
    "                          verbose = False) # do not want to spam output\n",
    "# Fit model\n",
    "model.fit(X_train.drop(\"candidate\", axis = 1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My cross_val_score results tell me that I am getting roughly 9% precision and roughly 11% recall. This is decent, but it is quite similar to my more naive approach, although the 17 feature vectors are much more compact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = cross_val_score(model, X_train, y_train, cv = 3, scoring = \"precision\")\n",
    "res2 = cross_val_score(model, X_train, y_train, cv = 3, scoring = \"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(res1.mean())\n",
    "display(res2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on my held-out (classification) data set prove to be very similar giving evidence that you can use either cross_val_score or validation and achieve similar results. (WARNING: results may vary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.98      0.99     11999\n",
      "        True       0.08      0.11      0.09       154\n",
      "\n",
      "    accuracy                           0.97     12153\n",
      "   macro avg       0.53      0.55      0.54     12153\n",
      "weighted avg       0.98      0.97      0.97     12153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only get an f1-score of .09 on the true values. This seems to be not the best results, however this is a difficult problems. Lets now look at the distribution of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    11929\n",
       "1.0      224\n",
       "dtype: int64"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predictions).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predicted many (224) to be the correct response and we only got 8% of those predictions correct, meaning that out of 224 that we predicted to be the correct answer only about 17 of them were actually the correct answer. In terms of recall we had 11% recall which means out of the 154 correct answers we correctly predicted only about 17 of them to be the correct answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I am only really paying attention to the values that were classified as being the correct answer since there were so many observations that were classified as being the incorrect answer we had 99% and 98% precision and recall respectively, so those results are not really of interest very much to me. Also, the goal of this challenge is to find **correct** answers not incorrect answers so from here on out when I refer to recall or precision I am talking about for the correct answer group not the incorrect answer group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prints a list of the indexes of the correct values, if those were ever to be of need. I have learned the hard way that often times observations that the machine picks out as being outliers end up just being ordinary observations for us to see and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_r = np.array(y_test * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == 1.0:\n",
    "        if predictions[i] == y_test_r[i] :\n",
    "            correct.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1962,\n",
       " 1980,\n",
       " 2587,\n",
       " 2953,\n",
       " 3548,\n",
       " 6008,\n",
       " 6239,\n",
       " 6331,\n",
       " 6338,\n",
       " 7009,\n",
       " 7365,\n",
       " 7954,\n",
       " 8332,\n",
       " 8849,\n",
       " 9640,\n",
       " 9857,\n",
       " 11520]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that indexing into the large data frame actually becomes significantly more difficult when using dask since dask automatically splits the data into many groups. I might come back to this however, the exact questions may not be the most interesting thing, especially given how normal the extreme outliers seemed to be in my second objective analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some strange reason it seems like catboost is appending the candidate variable back onto X_train so I will have to remove it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(\"candidate\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf2 = LogisticRegression(random_state=0, solver='lbfgs',class_weight=\"balanced\")\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.69      0.81     11999\n",
      "        True       0.03      0.75      0.06       154\n",
      "\n",
      "    accuracy                           0.69     12153\n",
      "   macro avg       0.51      0.72      0.44     12153\n",
      "weighted avg       0.98      0.69      0.81     12153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using logistic regression we get a lower precision for the true group however we actually improve in terms of our recall. It appears as if we are predicting more observations to be true than the cat boost was. However, our f1 went down which suggests that we are doing worse over all with logistic regression as compared to catboost.\n",
    "\n",
    "I am going to see how many we classified as successes to contrast with catboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8313\n",
       "1    3840\n",
       "dtype: int64"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predictions).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty bad, we predicted 3,840 to be true observations when only 154 were correct observations in reality. This seems to be the only model that predicts such a high number of observations to be in the correct question class I think that it would be worthwhile to do some hyperparameter tuning with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know that random forest classifiers are another good algorithm for classifying so I am going to give it a shot here with 100 parameters. We did not cover these in this class so I am not going to bother worrying about the hyper-parameters too much and instead I will just set the n_estimators to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100)\n",
    "# we want many estimators, this is a difficult problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99     11999\n",
      "        True       0.60      0.02      0.04       154\n",
      "\n",
      "    accuracy                           0.99     12153\n",
      "   macro avg       0.79      0.51      0.52     12153\n",
      "weighted avg       0.98      0.99      0.98     12153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random forest we actually ended up with a lower f1 score on our true predictions. Let me check the ratio of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12148\n",
       "1        5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only predicted 5 observations to be true and out of those only three of them were actually the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99     11999\n",
      "        True       0.00      0.00      0.00       154\n",
      "\n",
      "    accuracy                           0.99     12153\n",
      "   macro avg       0.49      0.50      0.50     12153\n",
      "weighted avg       0.97      0.99      0.98     12153\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, class_weight = \"balanced\")\n",
    "rf.fit(X_train, y_train) \n",
    "pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite surprisingly we get worse results for when we set the class weights to balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel = \"rbf\", gamma = \"auto\", class_weight = \"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.92      0.95     11999\n",
      "        True       0.02      0.11      0.03       154\n",
      "\n",
      "    accuracy                           0.91     12153\n",
      "   macro avg       0.50      0.52      0.49     12153\n",
      "weighted avg       0.98      0.91      0.94     12153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc.fit(X_train, y_train)\n",
    "pred = svc.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing seems to do as good as catboost did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.92      0.95     11999\n",
      "        True       0.02      0.11      0.03       154\n",
      "\n",
      "    accuracy                           0.91     12153\n",
      "   macro avg       0.50      0.52      0.49     12153\n",
      "weighted avg       0.98      0.91      0.94     12153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb.fit(X_train, y_train)\n",
    "pred = svc.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing has seemed to work thus far so it is time to break out the big guns. Time to use neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.78      0.87     11999\n",
      "        True       0.02      0.43      0.05       154\n",
      "\n",
      "    accuracy                           0.77     12153\n",
      "   macro avg       0.51      0.60      0.46     12153\n",
      "weighted avg       0.98      0.77      0.86     12153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = nn.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was disapointing, also neural networks do not have settings for class weights so I do not really know where to go from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear and quadratic discriminant analysis are two more machines learning methods that I am somewhat familiar with that I know are mathematically similar to logistic regression, which we get good results with so I will give them a shot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.99      0.99     11999\n",
      "        True       0.10      0.07      0.08       154\n",
      "\n",
      "    accuracy                           0.98     12153\n",
      "   macro avg       0.54      0.53      0.54     12153\n",
      "weighted avg       0.98      0.98      0.98     12153\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "pred = lda.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are actually quite surprising results. We get a f1-score of .08 which is just one shy of catboost. Sometimes the more simple models are actually the best. I am getting a warning saying that my variables are collinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.97      0.98     11999\n",
      "        True       0.02      0.06      0.03       154\n",
      "\n",
      "    accuracy                           0.95     12153\n",
      "   macro avg       0.50      0.51      0.50     12153\n",
      "weighted avg       0.98      0.95      0.96     12153\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "pred = qda.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our performance decreased when we switched to quadratic discriminant analysis. Let me investigate the covariant variables and see if that could possibly be bringing the performance of our model down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.99      0.99     11999\n",
      "        True       0.10      0.07      0.08       154\n",
      "\n",
      "    accuracy                           0.98     12153\n",
      "   macro avg       0.54      0.53      0.54     12153\n",
      "weighted avg       0.98      0.98      0.98     12153\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(store_covariance = True)\n",
    "lda.fit(X_train, y_train)\n",
    "pred = lda.predict(X_test)\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_frame = pd.DataFrame(lda.covariance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.022423</td>\n",
       "      <td>0.012725</td>\n",
       "      <td>0.014519</td>\n",
       "      <td>0.018213</td>\n",
       "      <td>0.012027</td>\n",
       "      <td>0.013058</td>\n",
       "      <td>7.113458e-01</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>-0.021599</td>\n",
       "      <td>-0.012725</td>\n",
       "      <td>-0.013695</td>\n",
       "      <td>0.071852</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>-0.039789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.012725</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.010402</td>\n",
       "      <td>0.030564</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>1.617064e+01</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>-0.000565</td>\n",
       "      <td>-0.000559</td>\n",
       "      <td>-0.012777</td>\n",
       "      <td>-0.032800</td>\n",
       "      <td>-0.003993</td>\n",
       "      <td>0.073661</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>-0.035919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.014519</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.029483</td>\n",
       "      <td>0.011584</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>-8.060546e+01</td>\n",
       "      <td>0.005210</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.003942</td>\n",
       "      <td>-0.028869</td>\n",
       "      <td>0.082332</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>-0.151750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018213</td>\n",
       "      <td>0.010402</td>\n",
       "      <td>0.011584</td>\n",
       "      <td>0.015047</td>\n",
       "      <td>0.009865</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>4.102463e-01</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>-0.000723</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.017921</td>\n",
       "      <td>-0.010402</td>\n",
       "      <td>-0.011292</td>\n",
       "      <td>0.061891</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>-0.029980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012027</td>\n",
       "      <td>0.030564</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.009865</td>\n",
       "      <td>0.028714</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>1.423494e+01</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>-0.000469</td>\n",
       "      <td>-0.012074</td>\n",
       "      <td>-0.030564</td>\n",
       "      <td>-0.003889</td>\n",
       "      <td>0.081718</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>-0.031760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.013058</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.025213</td>\n",
       "      <td>-8.995647e+01</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>-0.012944</td>\n",
       "      <td>-0.003609</td>\n",
       "      <td>-0.026860</td>\n",
       "      <td>0.086105</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>-0.136925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.711346</td>\n",
       "      <td>16.170644</td>\n",
       "      <td>-80.605457</td>\n",
       "      <td>0.410246</td>\n",
       "      <td>14.234941</td>\n",
       "      <td>-89.956475</td>\n",
       "      <td>4.510856e+06</td>\n",
       "      <td>-56.904108</td>\n",
       "      <td>-23.751331</td>\n",
       "      <td>-23.781430</td>\n",
       "      <td>-0.120145</td>\n",
       "      <td>-16.170644</td>\n",
       "      <td>81.196659</td>\n",
       "      <td>73.081185</td>\n",
       "      <td>1.760234</td>\n",
       "      <td>1523.723746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.005210</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>-5.690411e+01</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.005170</td>\n",
       "      <td>-0.022425</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>-0.120295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>-0.000565</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>-0.000723</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>-2.375133e+01</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>-0.000527</td>\n",
       "      <td>-0.000710</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>-0.028294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>-0.000559</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.000469</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>-2.378143e+01</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>-0.000532</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>-0.028319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>-0.021599</td>\n",
       "      <td>-0.012777</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>-0.017921</td>\n",
       "      <td>-0.012074</td>\n",
       "      <td>-0.012944</td>\n",
       "      <td>-1.201446e-01</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.021736</td>\n",
       "      <td>0.012777</td>\n",
       "      <td>0.014042</td>\n",
       "      <td>-0.070776</td>\n",
       "      <td>-0.001478</td>\n",
       "      <td>0.045014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>-0.012725</td>\n",
       "      <td>-0.032800</td>\n",
       "      <td>-0.003942</td>\n",
       "      <td>-0.010402</td>\n",
       "      <td>-0.030564</td>\n",
       "      <td>-0.003609</td>\n",
       "      <td>-1.617064e+01</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.012777</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>-0.073661</td>\n",
       "      <td>-0.003427</td>\n",
       "      <td>0.035919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>-0.013695</td>\n",
       "      <td>-0.003993</td>\n",
       "      <td>-0.028869</td>\n",
       "      <td>-0.011292</td>\n",
       "      <td>-0.003889</td>\n",
       "      <td>-0.026860</td>\n",
       "      <td>8.119666e+01</td>\n",
       "      <td>-0.005170</td>\n",
       "      <td>-0.000527</td>\n",
       "      <td>-0.000532</td>\n",
       "      <td>0.014042</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>0.029216</td>\n",
       "      <td>-0.081257</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>0.156974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.071852</td>\n",
       "      <td>0.073661</td>\n",
       "      <td>0.082332</td>\n",
       "      <td>0.061891</td>\n",
       "      <td>0.081718</td>\n",
       "      <td>0.086105</td>\n",
       "      <td>7.308119e+01</td>\n",
       "      <td>-0.022425</td>\n",
       "      <td>-0.000710</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>-0.070776</td>\n",
       "      <td>-0.073661</td>\n",
       "      <td>-0.081257</td>\n",
       "      <td>9.486909</td>\n",
       "      <td>0.134749</td>\n",
       "      <td>1.361137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>1.760234e+00</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>-0.001478</td>\n",
       "      <td>-0.003427</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>0.134749</td>\n",
       "      <td>0.059133</td>\n",
       "      <td>0.026725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>-0.039789</td>\n",
       "      <td>-0.035919</td>\n",
       "      <td>-0.151750</td>\n",
       "      <td>-0.029980</td>\n",
       "      <td>-0.031760</td>\n",
       "      <td>-0.136925</td>\n",
       "      <td>1.523724e+03</td>\n",
       "      <td>-0.120295</td>\n",
       "      <td>-0.028294</td>\n",
       "      <td>-0.028319</td>\n",
       "      <td>0.045014</td>\n",
       "      <td>0.035919</td>\n",
       "      <td>0.156974</td>\n",
       "      <td>1.361137</td>\n",
       "      <td>0.026725</td>\n",
       "      <td>4.195456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2         3          4          5  \\\n",
       "0   0.022423   0.012725   0.014519  0.018213   0.012027   0.013058   \n",
       "1   0.012725   0.032800   0.003942  0.010402   0.030564   0.003609   \n",
       "2   0.014519   0.003942   0.029483  0.011584   0.003842   0.026975   \n",
       "3   0.018213   0.010402   0.011584  0.015047   0.009865   0.010648   \n",
       "4   0.012027   0.030564   0.003842  0.009865   0.028714   0.003558   \n",
       "5   0.013058   0.003609   0.026975  0.010648   0.003558   0.025213   \n",
       "6   0.711346  16.170644 -80.605457  0.410246  14.234941 -89.956475   \n",
       "7   0.000303   0.000118   0.005210  0.000170   0.000151   0.004778   \n",
       "8  -0.000897  -0.000565   0.000547 -0.000723  -0.000475   0.000558   \n",
       "9  -0.000891  -0.000559   0.000552 -0.000719  -0.000469   0.000563   \n",
       "10 -0.021599  -0.012777  -0.013905 -0.017921  -0.012074  -0.012944   \n",
       "11 -0.012725  -0.032800  -0.003942 -0.010402  -0.030564  -0.003609   \n",
       "12 -0.013695  -0.003993  -0.028869 -0.011292  -0.003889  -0.026860   \n",
       "13  0.071852   0.073661   0.082332  0.061891   0.081718   0.086105   \n",
       "14  0.001656   0.003427   0.001093  0.001569   0.003887   0.001061   \n",
       "15 -0.039789  -0.035919  -0.151750 -0.029980  -0.031760  -0.136925   \n",
       "\n",
       "               6          7          8          9        10         11  \\\n",
       "0   7.113458e-01   0.000303  -0.000897  -0.000891 -0.021599  -0.012725   \n",
       "1   1.617064e+01   0.000118  -0.000565  -0.000559 -0.012777  -0.032800   \n",
       "2  -8.060546e+01   0.005210   0.000547   0.000552 -0.013905  -0.003942   \n",
       "3   4.102463e-01   0.000170  -0.000723  -0.000719 -0.017921  -0.010402   \n",
       "4   1.423494e+01   0.000151  -0.000475  -0.000469 -0.012074  -0.030564   \n",
       "5  -8.995647e+01   0.004778   0.000558   0.000563 -0.012944  -0.003609   \n",
       "6   4.510856e+06 -56.904108 -23.751331 -23.781430 -0.120145 -16.170644   \n",
       "7  -5.690411e+01   0.007380   0.003153   0.003156 -0.000264  -0.000118   \n",
       "8  -2.375133e+01   0.003153   0.002863   0.002869  0.000917   0.000565   \n",
       "9  -2.378143e+01   0.003156   0.002869   0.002891  0.000912   0.000559   \n",
       "10 -1.201446e-01  -0.000264   0.000917   0.000912  0.021736   0.012777   \n",
       "11 -1.617064e+01  -0.000118   0.000565   0.000559  0.012777   0.032800   \n",
       "12  8.119666e+01  -0.005170  -0.000527  -0.000532  0.014042   0.003993   \n",
       "13  7.308119e+01  -0.022425  -0.000710  -0.000818 -0.070776  -0.073661   \n",
       "14  1.760234e+00   0.002935   0.002790   0.002793 -0.001478  -0.003427   \n",
       "15  1.523724e+03  -0.120295  -0.028294  -0.028319  0.045014   0.035919   \n",
       "\n",
       "           12         13        14           15  \n",
       "0   -0.013695   0.071852  0.001656    -0.039789  \n",
       "1   -0.003993   0.073661  0.003427    -0.035919  \n",
       "2   -0.028869   0.082332  0.001093    -0.151750  \n",
       "3   -0.011292   0.061891  0.001569    -0.029980  \n",
       "4   -0.003889   0.081718  0.003887    -0.031760  \n",
       "5   -0.026860   0.086105  0.001061    -0.136925  \n",
       "6   81.196659  73.081185  1.760234  1523.723746  \n",
       "7   -0.005170  -0.022425  0.002935    -0.120295  \n",
       "8   -0.000527  -0.000710  0.002790    -0.028294  \n",
       "9   -0.000532  -0.000818  0.002793    -0.028319  \n",
       "10   0.014042  -0.070776 -0.001478     0.045014  \n",
       "11   0.003993  -0.073661 -0.003427     0.035919  \n",
       "12   0.029216  -0.081257 -0.000915     0.156974  \n",
       "13  -0.081257   9.486909  0.134749     1.361137  \n",
       "14  -0.000915   0.134749  0.059133     0.026725  \n",
       "15   0.156974   1.361137  0.026725     4.195456  "
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unfortunate part about covariance is that it depends on the scales of the variables and so it is somewhat hard to see which two variables are \"collinear\" as the warning states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now going to use grid search to try to tune the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:450: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:450: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:450: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:450: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:450: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:450: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:450: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:450: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:450: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:450: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('lda',\n",
       "                                        LinearDiscriminantAnalysis(n_components=None,\n",
       "                                                                   priors=None,\n",
       "                                                                   shrinkage=None,\n",
       "                                                                   solver='svd',\n",
       "                                                                   store_covariance=False,\n",
       "                                                                   tol=0.0001))],\n",
       "                                verbose=False),\n",
       "             iid=False, n_jobs=1,\n",
       "             param_grid={'lda__priors': [None, [1, 80]],\n",
       "                         'lda__solver': ['svd', 'lsqr']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('lda', LinearDiscriminantAnalysis()) # it was not letting me not use a pipe for some reason\n",
    "])\n",
    "\n",
    "\n",
    "## GRID SEARCH\n",
    "solvers = [\"svd\", \"lsqr\"]\n",
    "priors = [None, [1,80]]\n",
    "param_grid = {\n",
    "        'lda__solver': solvers,\n",
    "        'lda__priors': priors\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid, iid=False,scoring='f1',return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06593272, 0.06593272, 0.02476439, 0.02705605])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to get better results without using class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this strategy of vector storage is more efficient I am going to use many features and see which ones optimize logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mdbarshay/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('log',\n",
       "                                        LogisticRegression(C=1.0,\n",
       "                                                           class_weight=None,\n",
       "                                                           dual=False,\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,\n",
       "                                                           l1_ratio=None,\n",
       "                                                           max_iter=100,\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='warn',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid=False, n_jobs=1,\n",
       "             param_grid={'log__penalty': ['l1', 'l2'],\n",
       "                         'log__solver': ['liblinear']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('log', LogisticRegression()) # it was not letting me not use a pipe for some reason\n",
    "])\n",
    "\n",
    "\n",
    "## GRID SEARCH\n",
    "penalties = [\"l1\", \"l2\"]\n",
    "solvers = [\"liblinear\"]\n",
    "param_grid = {\n",
    "        'log__penalty': penalties,\n",
    "        'log__solver': solvers\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid, iid=False,scoring='f1',return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0057971, 0.0057971])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "mean_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out so many of these parameters are dependent on each other (you can only use one parameter with another specific parameter in many cases, check out the logistic regression documentation!). It doesn't really make much sense to use a grid search since grid search tries out every possibility. In addition I am starting to discover that the default settings are the defaults for a reason, and it seems like they get best results a good propoertion of the time. It turns out that failure can be a learning experience sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion/Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main takeaways that I can make from this project is that there is a lot to learn. I discovered through this analysis as well as my secondary results that you can have either 17 features or 17,000 features and you may get similar results in terms of f1 score (as I did). Another thing that I learned was that small changes that you make can have large impact down the line and often times it can be difficult or even impossible to discover which of those little changes that you make is going to yield the best results. Sometimes there is no explanation for the results that you have you just have to see what the error says and go with it. As I said in the cell above the defaults are good and grid search is not always the go to tool for hyper-parameter tuning. Based on my results in this project I have found that the choice of the *model* seems to be more important than the choice of the hyper-parameters as some of my models predicted no observations to be successes. I also find it interesting that I have such an objective opinion on whether something turns out to be a success or not if there are no metrics available to use. For example when a clustering algorithm produces results that I was not expecting or was not hoping for my opinion is to say that it is not doing a good job, however there is a chance that the computer is picking up on something that I am unable to."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
